# LLM-iSTFT-VITS

**Language Model-based Text-to-Speech with iSTFT Vocoder**

Pretrained GPT-2 ê¸°ë°˜ ê³ í’ˆì§ˆ ìŒì„± í•©ì„± ì‹œìŠ¤í…œ

---

## ğŸ¯ Overview

ì´ í”„ë¡œì íŠ¸ëŠ” **Pretrained Language Model (GPT-2)**ê³¼ **Neural Audio Codec (EnCodec)**, ê·¸ë¦¬ê³  **ê³ ì† iSTFT Vocoder**ë¥¼ ê²°í•©í•œ end-to-end TTS ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### í•µì‹¬ ì•„ì´ë””ì–´

1. **LLM as Sequence Predictor**: GPT-2ì˜ ê°•ë ¥í•œ sequence modeling ëŠ¥ë ¥ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ì˜¤ë””ì˜¤ í† í°ìœ¼ë¡œ ì§ì ‘ ë§¤í•‘
2. **Audio Tokenization**: EnCodecìœ¼ë¡œ ì˜¤ë””ì˜¤ë¥¼ discrete tokensë¡œ í‘œí˜„í•˜ì—¬ LLMì´ ì²˜ë¦¬ ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
3. **Fast Waveform Generation**: Multiband iSTFT Generatorë¡œ ì‹¤ì‹œê°„ê¸‰ ê³ í’ˆì§ˆ ìŒì„± ìƒì„±
4. **Transfer Learning**: Pretrained GPT-2 weightsë¥¼ í™œìš©í•˜ì—¬ ë¹ ë¥¸ ìˆ˜ë ´ê³¼ ì•ˆì •ì ì¸ í•™ìŠµ

---

## ğŸš€ ì£¼ìš” íŠ¹ì§•

### ëª¨ë¸ ì•„í‚¤í…ì²˜

- **Pretrained GPT-2 Backbone**: 117M íŒŒë¼ë¯¸í„° transformer (768 hidden, 12 layers, 12 heads)
- **Audio Codec**: EnCodec (8 codebooks, 1024 vocab, 50Hz frame rate)
- **Neural Vocoder**: Multiband iSTFT Generator (4 subbands)
- **End-to-End**: í…ìŠ¤íŠ¸ì—ì„œ íŒŒí˜•ê¹Œì§€ ë‹¨ì¼ ëª¨ë¸ë¡œ í•™ìŠµ

### ê¸°ìˆ ì  ì¥ì 

- **Transfer Learning**: GPT-2ì˜ ì‚¬ì „í•™ìŠµëœ ì–¸ì–´ ì´í•´ ëŠ¥ë ¥ í™œìš©
- **Memory Efficient**: Gradient checkpointingìœ¼ë¡œ 12GB GPUì—ì„œ í•™ìŠµ ê°€ëŠ¥
- **Fast Inference**: iSTFT ê¸°ë°˜ vocoderë¡œ ë¹ ë¥¸ ìŒì„± ìƒì„±
- **Multilingual Support**: Universal phonetic representation (IPA) ì§€ì›
- **Offline Mode**: ëª¨ë“  ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ ë¡œë“œ ê°€ëŠ¥

---

## ğŸ“Š ì‹œìŠ¤í…œ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Input  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text Tokenizer     â”‚ (IPA: 131 symbols)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GPT-2 Transformer  â”‚ (Pretrained, 768-d, 12 layers)
â”‚  + New Embedding    â”‚ (IPA vocab)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audio Tokens       â”‚ (EnCodec discrete tokens)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Decoder Network    â”‚ (Tokens â†’ Mel features)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  iSTFT Vocoder      â”‚ (Mel â†’ Waveform)
â”‚  (Multiband)        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Output    â”‚ (22.05 kHz audio)
â”‚   Audio     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í•™ìŠµ ê³¼ì •

```
Ground Truth Audio â†’ EnCodec â†’ Audio Tokens (target)
                                    â†‘
Text â†’ GPT-2 â†’ Predicted Logits â”€â”€â”˜ (Cross-entropy loss)
              â†“
         Audio Tokens â†’ Decoder â†’ Mel â†’ Vocoder â†’ Waveform
                                   â†“              â†“
                              Mel Loss       GAN Loss
                              STFT Loss      FM Loss
```

---

## ğŸ”§ Model Specifications

| Component | Specification |
|-----------|--------------|
| **LLM** | GPT-2 (117M params) |
| Hidden Size | 768 |
| Layers | 12 |
| Attention Heads | 12 |
| **Audio Codec** | EnCodec |
| Codebooks | 8 |
| Vocab Size | 1024 |
| Frame Rate | 50 Hz |
| **Vocoder** | Multiband iSTFT |
| Subbands | 4 |
| FFT Size | 16 |
| Hop Size | 4 |
| **Audio** | 22.05 kHz, 80 Mel channels |

---

## ğŸ› ï¸ Installation

### Requirements

```bash
pip install torch torchaudio
pip install transformers soundfile scipy
pip install phonemizer  # For text preprocessing
```

### Download Pretrained Models

#### 1. GPT-2 Model

```bash
python -c "from transformers import GPT2Model, GPT2Tokenizer; \
    model = GPT2Model.from_pretrained('gpt2'); \
    model.save_pretrained('./pretrained_llm/gpt2'); \
    tok = GPT2Tokenizer.from_pretrained('gpt2'); \
    tok.save_pretrained('./pretrained_llm/gpt2')"
```

#### 2. EnCodec

EnCodec ëª¨ë¸ì€ `./encodec_pretrained`ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“š Data Preparation

### Step 1: Prepare Text-Audio Pairs

Filelist í˜•ì‹:
```
/path/to/audio1.wav|Hello world
/path/to/audio2.wav|How are you
```

### Step 2: Convert Text to Phonemes (IPA)

```bash
# Preview first 5 samples
python3 preprocess_ipa.py -i input.txt -o output.txt -l en-us --preview

# Full conversion
python3 preprocess_ipa.py -i input.txt -o output_ipa.txt -l en-us
```

**Supported Languages:**
- `en-us`: English (US)
- `ko`: Korean
- `ja`: Japanese
- `cmn`: Chinese (Mandarin)
- `es`: Spanish
- `fr`: French
- `de`: German
- More: See [espeak languages](https://github.com/espeak-ng/espeak-ng/blob/master/docs/languages.md)

**Output:**
```
/path/to/audio1.wav|hÉ™loÊŠ wÉœËld
/path/to/audio2.wav|haÊŠ É‘ËÉ¹ juË
```

---

## ğŸ“ Training

### Configuration

Edit `configs/ipa_tts.json`:

```json
{
  "train": {
    "log_interval": 10,
    "save_interval": 100,     // Checkpoint save frequency
    "batch_size": 2,          // Adjust based on GPU memory
    "learning_rate": 2e-4,
    "c_llm": 1.0,             // LLM loss weight
    "c_mel": 45,              // Mel reconstruction loss weight
    "c_fm": 2.0,              // Feature matching loss weight
    "c_stft": 1.0             // STFT loss weight
  },
  "model": {
    "hidden_size": 768,       // GPT-2 hidden (fixed)
    "n_layers": 12,           // GPT-2 layers (fixed)
    "n_heads": 12             // GPT-2 heads (fixed)
  }
}
```

### Start Training

```bash
python3 train_ipa.py -c configs/ipa_tts.json -m my_model
```

**Output Structure:**
```
logs/
â””â”€â”€ my_model/
    â”œâ”€â”€ train.log              # Training log
    â””â”€â”€ events.out.tfevents.*  # TensorBoard events

checkpoints/
â””â”€â”€ my_model/
    â”œâ”€â”€ G_init.pth            # Initial generator
    â”œâ”€â”€ D_init.pth            # Initial discriminator
    â”œâ”€â”€ G_step100.pth         # Generator at step 100
    â”œâ”€â”€ D_step100.pth         # Discriminator at step 100
    â””â”€â”€ ...
```

**Checkpoints:**
- Saved every `save_interval` steps (default: 50)
- Location: `checkpoints/my_model/G_step*.pth`, `D_step*.pth`
- Logs: `logs/my_model/train.log`
- TensorBoard: `tensorboard --logdir logs/`

### Loss Functions

The model is trained with 6 loss functions:

1. **LLM Loss**: Cross-entropy between predicted and target audio tokens
2. **Mel Loss**: L1 loss between predicted and ground truth mel spectrograms
3. **GAN Loss**: Adversarial loss for realistic waveform generation
4. **Feature Matching Loss**: Discriminator feature matching
5. **STFT Loss**: Multi-resolution STFT loss for audio quality
6. **Discriminator Loss**: Real/fake discrimination

**Total Generator Loss:**
```
L_G = c_llm Ã— L_LLM + c_mel Ã— L_Mel + L_GAN + c_fm Ã— L_FM + c_stft Ã— L_STFT
```

---

## ğŸ¤ Inference

### Method 1: Auto Text-to-IPA Conversion

```python
from synthesize_ipa import synthesize_text

# Korean
synthesize_text("ì•ˆë…•í•˜ì„¸ìš”", language='ko', output_path="output_ko.wav")

# English
synthesize_text("Hello world", language='en-us', output_path="output_en.wav")
```

### Method 2: Direct IPA Input

```python
from synthesize_ipa import synthesize

ipa_text = "hÉ™loÊŠ wÉœËld"
synthesize(ipa_text, checkpoint_path="ipa_tts/G_latest.pth", output_path="output.wav")
```

### Method 3: Command Line

```bash
python3 synthesize_ipa.py
```

**Output:**
- `output_auto_ko_*.wav`: Korean samples
- `output_auto_en-us_*.wav`: English samples

---

## ğŸ”¬ Technical Details

### Model Architecture

**1. Text Encoder (GPT-2-based)**
- Pretrained GPT-2 with **replaced embedding layer** for IPA vocab (131 symbols)
- All transformer weights kept from pretraining
- Gradient checkpointing enabled for memory efficiency

**2. Audio Tokenizer (EnCodec)**
- Neural audio codec with 8 codebooks
- Each codebook: 1024 discrete tokens
- Frame rate: 50 Hz (256 hop size at 22.05 kHz)

**3. Decoder Network**
- Projects audio tokens to mel features
- Input: One-hot encoded audio tokens (1024-d)
- Output: 80-channel mel spectrogram

**4. Vocoder (Multiband iSTFT)**
- 4-subband multiband processing
- ISTFT-based waveform generation
- No autoregressive sampling required (parallel generation)

### Training Strategy

- **Batch Size**: 2 (12GB GPU with gradient checkpointing)
- **Learning Rate**: 2e-4 with exponential decay (Î³=0.9999)
- **Optimizer**: AdamW (Î²1=0.9, Î²2=0.999)
- **Sequence**: Teacher forcing with ground truth audio tokens during training

### Memory Optimization

- **Gradient Checkpointing**: Enabled on GPT-2 transformer blocks
- **Batch Size 1-2**: Fits on 12GB GPU
- **Mixed Precision**: Optional (set `fp16_run: true` in config)

---

## ğŸ“ˆ Performance

| Metric | Value |
|--------|-------|
| Model Size | ~130M parameters |
| GPU Memory (Training) | ~10 GB (batch size 2) |
| Training Speed | ~0.5s/step (12GB GPU) |
| Inference Speed | Real-time+ (GPU) |
| Audio Quality | 22.05 kHz, high fidelity |

---

## ğŸ“ Project Structure

```
LLM-istft-vits/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ ipa_tts.json              # Training configuration
â”œâ”€â”€ pretrained_llm/
â”‚   â””â”€â”€ gpt2/                     # Pretrained GPT-2 model
â”œâ”€â”€ encodec_pretrained/           # EnCodec model (offline)
â”œâ”€â”€ filelists/                    # Training/validation filelists
â”œâ”€â”€ ipa_tokenizer.py              # IPA tokenizer (131 symbols)
â”œâ”€â”€ ipa_gpt2_model.py             # GPT-2-based TTS LLM
â”œâ”€â”€ model_complete_ipa.py         # Complete TTS pipeline
â”œâ”€â”€ audio_tokenizer.py            # EnCodec wrapper
â”œâ”€â”€ models.py                     # Vocoder (iSTFT Generator, Discriminator)
â”œâ”€â”€ mel_processing.py             # Mel spectrogram utilities
â”œâ”€â”€ data_utils_ipa.py             # Data loader
â”œâ”€â”€ train_ipa.py                  # Training script
â”œâ”€â”€ synthesize_ipa.py             # Inference script
â”œâ”€â”€ preprocess_ipa.py             # Text â†’ IPA preprocessing
â””â”€â”€ README.md                     # This file
```

---

## ğŸŒ Multilingual Support

The system uses **IPA (International Phonetic Alphabet)** as a universal phonetic representation, enabling true multilingual TTS.

**Process:**
1. Input text (any language) â†’ Phonemizer â†’ IPA
2. IPA â†’ GPT-2 â†’ Audio tokens
3. Audio tokens â†’ Waveform

**Supported Languages:**
English, Korean, Japanese, Chinese, Spanish, French, German, Russian, Italian, Portuguese, and more.

---

## ğŸ”— References

- **GPT-2**: [Language Models are Unsupervised Multitask Learners](https://github.com/openai/gpt-2)
- **EnCodec**: [High Fidelity Neural Audio Compression](https://github.com/facebookresearch/encodec)
- **iSTFT Vocoder**: [Multiband iSTFT Generator](https://github.com/rishikksh20/iSTFTNet-pytorch)
- **IPA**: [International Phonetic Alphabet](https://en.wikipedia.org/wiki/International_Phonetic_Alphabet)

---

## ğŸ“„ License

MIT License

---

## ğŸ™ Acknowledgments

- Pretrained GPT-2 from [Hugging Face Transformers](https://huggingface.co/gpt2)
- EnCodec from [Meta AI Research](https://github.com/facebookresearch/encodec)
- IPA phonemization via [phonemizer](https://github.com/bootphon/phonemizer)
